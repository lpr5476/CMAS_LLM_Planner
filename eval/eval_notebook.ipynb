{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b20312c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ba2ce36854469da42fd70f5d59864c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b781e940604be3b1dc868056a6dbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== WITH-CONTEXT ADAPTER =====\n",
      "Test file: lora_cmas_with_context.test.jsonl\n",
      "\n",
      "--- PROMPT (system + last user) ---\n",
      "[SYSTEM] You are a CMAS goal synthesis assistant. Use the provided Context to craft a single CMAS goal in the canonical simple form. Follow these rules: 1) Treat Context as authoritative when it conflicts with...\n",
      "[USER] Context:\n",
      "AGENTS\n",
      "- Crane      [RESOURCE] { Description: Resource to transport products, Interfaces: CraneInterface, Skills: transportPart, Variables: atX=0, atY=0, inUse=0, setX=0, setY=0, targetX=0, targetY=0, vacuum=0 }\n",
      "- Process1   [RESOURCE] { Description: Process 1, Interfaces: ProcessInterface, CraneInterface, Skills: RunProcess1, Variables: Location(x=478, y=107), OffsetLocation(x=478, y=216...\n",
      "\n",
      "--- MODEL OUTPUT ---\n",
      "ProductA: transportPart(OffsetFromLocation(55,229), fromLocation(54,74), offsetToLocation(450,216), toLocation(478,107)) ; RunProcess1 ; transportPart(OffsetFromLocation(450,216), fromLocation(478,107), offsetToLocation(650,224), toLocation(696,91)) ; RunProcess2 ; transportPart(OffsetFromLocation(650,224), fromLocation(696,91), offsetToLocation(945,225), toLocation(949,100)) ; packing\n",
      "\n",
      "--- GOLD (last example) ---\n",
      "ProductA: transportPart(OffsetFromLocation(55,229), fromLocation(54,74), offsetToLocation(450,216), toLocation(478,107)) ; RunProcess1 ; transportPart(OffsetFromLocation(450,216), fromLocation(478,107), offsetToLocation(650,224), toLocation(696,91)) ; RunProcess2 ; transportPart(OffsetFromLocation(650,224), fromLocation(696,91), offsetToLocation(945,225), toLocation(949,100)) ; packing <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33062417ea9d4a3da897bde1ea4eedd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== NO-CONTEXT ADAPTER =====\n",
      "Test file: lora_cmas_no_context.test.jsonl\n",
      "\n",
      "--- PROMPT (system + last user) ---\n",
      "[USER] To begin with, transport ProductA to the process area, then run Process 1. Next, transport ProductA once more and execute Process 2. Finally, pack ProductA.\n",
      "\n",
      "--- MODEL OUTPUT ---\n",
      "ProductA: transportPart(OffsetFromLocation(55,223), fromLocation(71,85), offsetToLocation(450,197), toLocation(478,108)) ; RunProcess1 ; transportPart(OffsetFromLocation(450,197), fromLocation(478,108), offsetToLocation(650,225), toLocation(662,105)) ; RunProcess2 ; transportPart(OffsetFromLocation(650,225), fromLocation(662,105), offsetToLocation(945,193), toLocation(964,98)) ; packing user\n",
      "\n",
      "ProductA: transportPart(OffsetFromLocation(55,223), fromLocation(71,85), offsetToLocation(450,197), toLocation(478,108)) ; RunProcess1 ; transportPart(OffsetFromLocation(450,197), fromLocation(478,108), offsetToLocation(650,225), toLocation(662,105)) ; RunProcess2 ; transportPart\n",
      "\n",
      "--- GOLD (last example) ---\n",
      "ProductA: transportPart(OffsetFromLocation(55,229), fromLocation(54,74), offsetToLocation(450,216), toLocation(478,107)) ; RunProcess1 ; transportPart(OffsetFromLocation(450,216), fromLocation(478,107), offsetToLocation(650,224), toLocation(696,91)) ; RunProcess2 ; transportPart(OffsetFromLocation(650,224), fromLocation(696,91), offsetToLocation(945,225), toLocation(949,100)) ; packing <|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# demo_infer_from_last_tests.py\n",
    "# Use local base model \"base_llm/\" + TWO LoRA adapters.\n",
    "# For EACH test JSONL, take the LAST example (more realistic), strip the gold assistant,\n",
    "# generate, and print model output next to the gold.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# ====== EDIT THESE PATHS ONLY ======\n",
    "BASE_DIR = \"base_llm/\"\n",
    "ADAPTER_WITH_CONTEXT = \"./lora_cmas_with_context\"\n",
    "ADAPTER_NO_CONTEXT   = \"./lora_cmas_no_context\"\n",
    "TEST_WITH_JSONL = \"lora_cmas_with_context.test.jsonl\"\n",
    "TEST_NO_JSONL   = \"lora_cmas_no_context.test.jsonl\"\n",
    "MAX_NEW_TOKENS = 200\n",
    "# ===================================\n",
    "\n",
    "def read_last_jsonl(path: str):\n",
    "    last_obj = None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            s = line.strip()\n",
    "            if s:\n",
    "                last_obj = json.loads(s)\n",
    "    if last_obj is None:\n",
    "        raise RuntimeError(f\"No examples found in: {path}\")\n",
    "    return last_obj\n",
    "\n",
    "def strip_assistant_tail(messages: List[Dict[str,str]]) -> List[Dict[str,str]]:\n",
    "    return messages[:-1] if messages and messages[-1].get(\"role\",\"\").lower()==\"assistant\" else messages\n",
    "\n",
    "def extract_gold(messages: List[Dict[str,str]]) -> str:\n",
    "    return messages[-1][\"content\"].strip() if messages and messages[-1].get(\"role\",\"\").lower()==\"assistant\" else \"\"\n",
    "\n",
    "def load_base_and_tokenizer():\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_DIR, use_fast=True, trust_remote_code=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_DIR,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    ).eval()\n",
    "    return tok, base\n",
    "\n",
    "def attach_adapter(base_model, adapter_dir: str):\n",
    "    return PeftModel.from_pretrained(base_model, adapter_dir).eval()\n",
    "\n",
    "def generate(tok, model, messages: List[Dict[str,str]]):\n",
    "    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            temperature=0.5,\n",
    "            do_sample=True,\n",
    "            eos_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    gen_ids = out[0, inputs[\"input_ids\"].shape[-1]:]\n",
    "    return tok.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_one(test_path: str, adapter_path: str, label: str, tok=None):\n",
    "    obj = read_last_jsonl(test_path)\n",
    "    full_msgs = obj[\"messages\"]\n",
    "    gold = extract_gold(full_msgs)\n",
    "    msgs = strip_assistant_tail(full_msgs)\n",
    "\n",
    "    # (Re)load base for each adapter to avoid clashes\n",
    "    if tok is None:\n",
    "        tok, base = load_base_and_tokenizer()\n",
    "    else:\n",
    "        _, base = load_base_and_tokenizer()\n",
    "    model = attach_adapter(base, adapter_path)\n",
    "\n",
    "    pred = generate(tok, model, msgs)\n",
    "\n",
    "    print(f\"\\n===== {label} =====\")\n",
    "    print(f\"Test file: {test_path}\")\n",
    "    # Show brief prompt context (system + last user)\n",
    "    sys = next((m[\"content\"] for m in msgs if m[\"role\"].lower()==\"system\"), \"\")\n",
    "    users = [m[\"content\"] for m in msgs if m[\"role\"].lower()==\"user\"]\n",
    "    print(\"\\n--- PROMPT (system + last user) ---\")\n",
    "    if sys:\n",
    "        print(f\"[SYSTEM] {sys[:200]}{'...' if len(sys)>200 else ''}\")\n",
    "    if users:\n",
    "        u = users[-1]\n",
    "        print(f\"[USER] {u[:400]}{'...' if len(u)>400 else ''}\")\n",
    "    print(\"\\n--- MODEL OUTPUT ---\")\n",
    "    print(pred if pred else \"(empty)\")\n",
    "    if gold:\n",
    "        print(\"\\n--- GOLD (last example) ---\")\n",
    "        print(gold[:400] + (\"...\" if len(gold)>400 else \"\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer, _ = load_base_and_tokenizer()  # load once; bases are reloaded per adapter\n",
    "    run_one(TEST_WITH_JSONL, ADAPTER_WITH_CONTEXT, \"WITH-CONTEXT ADAPTER\", tok=tokenizer)\n",
    "    run_one(TEST_NO_JSONL,   ADAPTER_NO_CONTEXT,   \"NO-CONTEXT ADAPTER\",   tok=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80365cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
